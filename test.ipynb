{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6bf891",
   "metadata": {},
   "source": [
    "# Проект большие языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa59ba",
   "metadata": {},
   "source": [
    "### Подготовка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61607157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast, LlamaConfig, LlamaForCausalLM, Trainer, TrainingArguments,TrainerCallback, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim import AdamW \n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e311b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE='cpu'\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"RussianNovels/corpus\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VOCAB_SIZE = 3000\n",
    "CYRILLIC_RE = re.compile(r\"^[\\p{IsCyrillic}\\s0-9.,!?—–:;\\\"'()«»…\\-]+$\")\n",
    "BATCH_SIZE = 2 if DEVICE=='cpu' else 8\n",
    "MAX_EPOCHS = 1\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP = 1.0\n",
    "SAVE_PATH = \"best_model.pt\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\"\n",
    "] \n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "QUESTIONS = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\"\n",
    "]\n",
    "\n",
    "print(f\"{DEVICE=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07dc670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'RussianNovels' already exists and is not an empty directory.\n",
      "fatal: destination path 'alpaca-cleaned-ru' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/JoannaBy/RussianNovels.git\n",
    "!git clone https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56db64e",
   "metadata": {},
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404b8f6",
   "metadata": {},
   "source": [
    "### Подготовка корпуска текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3b3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлов в корпусе: 107\n"
     ]
    }
   ],
   "source": [
    "def collect_texts(root: Path):\n",
    "    texts = []\n",
    "    for path in root.rglob(\"*\"):\n",
    "        if path.suffix in {\".txt\"}:\n",
    "            try:\n",
    "                texts.append(path.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return texts\n",
    "\n",
    "raw_texts = collect_texts(DATA_DIR)\n",
    "raw_texts = list(set(raw_texts))\n",
    "print(f\"Файлов в корпусе: {len(raw_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c3d8316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложений после очистки: 523962\n"
     ]
    }
   ],
   "source": [
    "def filter_cyrillic_sentences(text: str):\n",
    "    '''фильтруем предложения с некириллическими символами'''\n",
    "    sentences = re.split(r\"[.!?]+\", text)\n",
    "    return [\n",
    "        s.strip()\n",
    "        for s in sentences\n",
    "        if s.strip() and CYRILLIC_RE.match(s.strip())\n",
    "    ]\n",
    "\n",
    "def normalize_punctuation(text: str) -> str:\n",
    "    '''нормализуемы пунктуацию'''\n",
    "    text = re.sub(r\"[!?]{2,}\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\.{3,}\", \"…\", text)\n",
    "    text = re.sub(r\",,{2,}\", \",\", text)\n",
    "    text = re.sub(r\"-{2,}\", \"-\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r'[«»]', '\"', text)\n",
    "    return text.strip()    \n",
    "\n",
    "clean_sentences = []\n",
    "\n",
    "for text in raw_texts:\n",
    "    sentences = filter_cyrillic_sentences(text)\n",
    "    sentences = [normalize_punctuation(s) for s in sentences]\n",
    "    clean_sentences.extend(sentences)\n",
    "\n",
    "print(f\"Предложений после очистки: {len(clean_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6095aab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка состава оставшихся спец символов\n",
      "symb | freg    | name\n",
      "' '  | 5716309 | SPACE\n",
      "','  |  853248 | COMMA\n",
      "'-'  |  279089 | HYPHEN-MINUS\n",
      "'\"'  |   49998 | QUOTATION MARK\n",
      "':'  |   44922 | COLON\n",
      "';'  |   32563 | SEMICOLON\n",
      "'–'  |   24641 | EN DASH\n",
      "')'  |    7897 | RIGHT PARENTHESIS\n",
      "'('  |    7337 | LEFT PARENTHESIS\n",
      "'—'  |    4435 | EM DASH\n",
      "'…'  |    2988 | HORIZONTAL ELLIPSIS\n",
      "'''  |    1152 | APOSTROPHE\n"
     ]
    }
   ],
   "source": [
    "print(\"Проверка состава оставшихся спец символов\")\n",
    "def is_letter_or_digit(ch: str) -> bool:\n",
    "    cat = unicodedata.category(ch)\n",
    "    return cat.startswith(\"L\") or cat.startswith(\"N\")\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for sent in clean_sentences:\n",
    "    for ch in sent:\n",
    "        if not is_letter_or_digit(ch):\n",
    "            counter[ch] += 1\n",
    "\n",
    "\n",
    "non_alnum = counter.most_common()\n",
    "print(f\"symb | freg    | name\")\n",
    "for ch, freq in non_alnum[:30]:\n",
    "    name = unicodedata.name(ch, \"UNKNOWN\")\n",
    "    print(f\"'{ch}'  | {freq:>7} | {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152d4382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чанков: 20036\n",
      "чанк  0: Несмотря на то, что время клонилось къ вечеру, жаръ не спадалъ Такъ было тихо кругомъ ихъ, какъ-будто все замерло въ лѣсу Ни травка, ни одинъ листикъ не шелохнулись Лѣсъ такъ былъ густъ, что охотниковъ окружала со всѣхъ сторонъ плотная, высокая стѣна зелени, отнимавшая всякую возможность прохлады Одинъ изъ охотниковъ, щеголевато одѣтый, покрой платья котораго скорѣе былъ красивъ, чѣмъ удобенъ, снялъ бархатную небольшую фуражку и, тряхнувъ своими длинными, взмокшими волосами, съ сердцемъ сказалъ: - А все по твоей милости, я умираю отъ жажды и задыхаюсь отъ усталости Я говорилъ, взять провожатаго Нѣтъ, не согласился Вотъ теперь и блуждай по лѣсу И, взглянувъ на часы, онъ еще раздражительнѣе продолжалъ: - Скоро солнце сядетъ, а ужь ночью трудно по этому глухому лѣсу пробираться; я и такъ себѣ ноги всѣ разбилъ и все лицо перецарапалъ - Останемся ночевать, равнодушно замѣтилъ его товарищъ въ картузѣ съ козырькомъ, походившимъ на бекасиный носъ - Ночевать что ты, съ ума сошелъ Не ты ли мнѣ разсказывалъ, что здѣсь бездна медвѣдей и волковъ - Экой трусъ насмѣшливо замѣтилъ его товарищъ - Сейчасъ видно, что недавно изъ столицы Деревенскому жителю тутъ только и раздолье, какъ дѣло дойдетъ до звѣря - Ну, запѣлъ свою старую пѣсню Но я сегодня не намѣренъ спорить съ тобой о преимуществахъ столичной жизни Да и чѣмъ тугъ хвалиться, что ты не боишься дурацкой смерти отъ звѣря - Ну, а чѣмъ же умнѣе смерть:- застрѣлиться, напримѣръ, отъ любви, какъ ты хотѣлъ-было годъ тому назадъ а И охотникъ въ картузѣ съ бекасинымъ носомъ тихо засмѣялся Веселость его еще больше раздражила охотника въ бархатной фуражкѣ, и онъ спросилъ грубо: - Ну, чему тутъ смѣяться - Да какже не смѣяться: у васъ тамъ влюбятся, и если женщина разлюбила или не полюбила - бацъ и ноги протянулъ ха, ха, ха А знаешь, все это отчего происходитъ - Ну-ка, скажи Живи въ деревнѣ, ты бы въ два дни выздоровѣлъ отъ всякой такого рода болѣзни\n",
      "чанк 10: Камышовъ засталъ своего товарища бодрствующимъ, и, по его блѣдному лицу и усталымъ глазамъ, можно было догадаться, что онъ не спалъ во всю ночь Онъ пристально смотрѣлъ на Бѣлку, спавшую на сучьяхъ того же дерева, свернувшись какъ червякъ на листкѣ Зелень придавала ея миленькому личику какую-то мертвенность Ея золотистыя длинныя косы слегка колыхались внизу, упавъ между сучьями Камышовъ долго смотрѣлъ на Бѣлку и, обратясь къ своему товарищу, спросилъ: - Что ты не спишь Охотникъ въ картузѣ вздрогнулъ и недовольнымъ голосомъ отвѣчалъ: - Я ужь выспался - Я готовъ голову прозакладовать, что старикъ скрылъ много ей лѣтъ шопотомъ замѣтилъ Камышовъ - Не думаю: она еще совершенное дитя Вчера, какъ ты спалъ, я долго съ ней говорилъ, и убѣдился, что она замѣчательный ребенокъ - Вырвать ее изъ этой жизни быстро перебилъ его охотникъ въ картузѣ - Воспитай ее, сдѣлай изъ нея женщину хорошую - А потомъ язвительно спросилъ охотникъ въ картузѣ - Ну, потомъ женись, спокойно отвѣчалъ Камышовъ Его товарищъ принужденно засмѣялся и сказалъ: - Чтобъ осуществить глупаго доктора Бартоло Бѣлка пугливо вскочила и слабо вскрикнула; одна ея коса зацѣпилась за сучокъ Охотники кинулись-было помогать ей; но Бѣлка, раздвинувъ сучья, скользнула внизъ и исчезла Охотники переглянулись, и Камышовъ замѣтилъ: - Ей скучна будетъ наша жизнь Камышовъ послѣдовалъ его примѣру, и оба они погрузились въ сонъ Солнце, проникая сквозь зелень, жгучими своими лучами пробудило Камышова, который не нашелъ уже возлѣ себя своего товарища Спускаясь по крутой горѣ, онъ дивился, какъ это они, взбираясь сюда ночью, не сломили себѣ шеи Было уже около десяти часовъ Старикъ сидѣлъ у шалаша, грѣясь на солнышкѣ и задумчиво пощипывая свою сѣдую бороду При видѣ охотника, старикъ всталъ и низкимъ поклономъ встрѣтилъ его Охотникъ подсѣлъ къ старику и сталъ разспрашивать его о житьѣ и объ ульяхъ\n"
     ]
    }
   ],
   "source": [
    "# разбиение на чанки\n",
    "chunks = []\n",
    "current = \"\"\n",
    "\n",
    "for sent in clean_sentences:\n",
    "    max_chars = 2000  # ~512 токенов\n",
    "    if len(current) + len(sent) < max_chars:\n",
    "        current += \" \" + sent\n",
    "    else:\n",
    "        chunks.append(current.strip())\n",
    "        current = sent\n",
    "\n",
    "if current:\n",
    "    chunks.append(current.strip())\n",
    "\n",
    "print(f\"Чанков: {len(chunks)}\")\n",
    "print(f\"чанк  0: {chunks[0]}\")\n",
    "print(f\"чанк 10: {chunks[10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cb3e6",
   "metadata": {},
   "source": [
    "### Обучение токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49e3cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Нормализация текста\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "# разбиение по пробелам \n",
    "tokenizer.pre_tokenizer = Whitespace() \n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"] \n",
    "    )\n",
    "tokenizer.train_from_iterator(chunks, trainer)\n",
    "# (BOS/EOS) \n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    pair=\"[BOS] $A [EOS] $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")), \n",
    "        ], \n",
    "        )\n",
    "# Сохранение\n",
    "tokenizer.save(\"bpe_ru_3k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8d1788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', 'пример', 'те', 'к', 'ста', 'для', 'про', 'вер', 'ки', 'то', 'ке', 'ни', 'за', 'тора', '[EOS]']\n",
      "[2, 1903, 80, 31, 123, 308, 106, 170, 100, 58, 220, 70, 81, 1680, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"Пример текста для проверки токенизатора\")\n",
    "print(encoded.tokens)\n",
    "print(encoded.ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8334a89",
   "metadata": {},
   "source": [
    "### Сборка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c6c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e0e093b04044b99a5fd0de15226a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20036 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145d5f475ece4a0384dc58b9b2743797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20036 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"bpe_ru_3k.json\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    ")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return hf_tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "def add_labels(batch):\n",
    "    labels = []\n",
    "    for ids, mask in zip(batch[\"input_ids\"], batch[\"attention_mask\"]):\n",
    "        l = ids.copy()\n",
    "        l = [\n",
    "            token if m == 1 else -100\n",
    "            for token, m in zip(l, mask)\n",
    "        ]\n",
    "        labels.append(l)\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": chunks})\n",
    "\n",
    "dataset = dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ").map(add_labels, batched=True)\n",
    "\n",
    "dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c5204",
   "metadata": {},
   "source": [
    "### Подготовка модели LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62fe2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучаемых параметров: 128,934,912\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=2048,\n",
    "    rms_norm_eps=1e-6,\n",
    "    hidden_act=\"silu\",\n",
    "    tie_word_embeddings=True,\n",
    "    attention_bias=False,\n",
    "    rope_theta=10000.0,\n",
    "    use_cache=False,     \n",
    ")\n",
    "config.vocab_size = max(hf_tokenizer.get_vocab().values()) + 1\n",
    "config.pad_token_id = hf_tokenizer.pad_token_id\n",
    "config.bos_token_id = hf_tokenizer.bos_token_id\n",
    "config.eos_token_id = hf_tokenizer.eos_token_id\n",
    "\n",
    "model = LlamaForCausalLM(config).to(DEVICE)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Обучаемых параметров: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81028a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверим синхронизацию модели и текенизатора\n",
      "ок\n"
     ]
    }
   ],
   "source": [
    "print(\"Проверим синхронизацию модели и текенизатора\")\n",
    "assert model.config.vocab_size == max(hf_tokenizer.get_vocab().values()) + 1\n",
    "assert model.config.pad_token_id == hf_tokenizer.pad_token_id\n",
    "assert model.config.eos_token_id == hf_tokenizer.eos_token_id\n",
    "print(\"ок\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d2a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity-check начальной модели на инференсе\n",
      "при вет кры кры брат брат брат брат брат брат брат брат бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары\n"
     ]
    }
   ],
   "source": [
    "print(\"Sanity-check начальной модели на инференсе\")\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Привет\"\n",
    "inputs = hf_tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    return_token_type_ids=False\n",
    ").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=False,\n",
    "        eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(hf_tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ada5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимизатор\n",
    "def get_optim():\n",
    "    return AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optimizer = get_optim()\n",
    "\n",
    "# освобождение памяти\n",
    "def gc_collect():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b35f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сheck сходимости модели при обучение на малом датасете (2 предожения)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d2303850cc4f19b646a0a550bcbfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train check:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  20 loss: 5.32\n",
      "best model saved with best_loss=5.32\n",
      "epoch:  40 loss: 5.04\n",
      "best model saved with best_loss=5.04\n",
      "epoch:  60 loss: 4.73\n",
      "best model saved with best_loss=4.73\n",
      "epoch:  80 loss: 4.40\n",
      "best model saved with best_loss=4.40\n",
      "epoch: 100 loss: 2.32\n",
      "best model saved with best_loss=2.32\n",
      "epoch: 120 loss: 0.12\n",
      "best model saved with best_loss=0.12\n",
      "epoch: 140 loss: 0.01\n",
      "best model saved with best_loss=0.01\n",
      "epoch: 160 loss: 0.00\n",
      "best model saved with best_loss=0.00\n",
      "epoch: 180 loss: 0.00\n",
      "epoch: 200 loss: 0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"Сheck сходимости модели при обучении на малом датасете (2 предожения)\")\n",
    "tiny_dataset = dataset.select(range(2))\n",
    "tiny_loader = DataLoader(tiny_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "progress_bar = tqdm(\n",
    "        range(201),\n",
    "        desc=f\"train check\",\n",
    "    )\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for step in progress_bar:\n",
    "    batch = next(iter(tiny_loader))\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "    loss = model(**batch).loss\n",
    "    progress_bar.set_description(f\"train check | loss: {loss:.2f}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step> 0 and step % 20 == 0:\n",
    "        progress_bar.write(f\"epoch: {step:>3} loss: {loss.item():.2f}\")\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "            progress_bar.write(f\"best model saved with {best_loss=:.2f}\")\n",
    "\n",
    "del tiny_dataset, tiny_loader, optimizer\n",
    "gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "614f969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сheck инференса модели после обучения на малом датасете\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия къ вече ру , жар ъ не спа дал ъ не х ъ их ъ их ъ их ъ , какъ - будто все заме р ло въ л ѣ су ни тра в ка , ни один ъ ли сти къ не ше ло х нулись л ѣ съ\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха время кло кло , жар ъ не спа дал ъ не въ л ѣ су ни с ха ст ѣ ле ъ его заме р ло въ л ѣ су ни тра в ка , ни один ъ ли сти къ не ше ло х нулись л ѣ съ так\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Сheck инференса модели после обучения на малом датасете\")\n",
    "def check_inference(model, prompts: list):\n",
    "    model.eval()\n",
    "    for pr in prompts:\n",
    "        inputs = hf_tokenizer(pr, return_tensors=\"pt\").to(model.device)\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "        out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True, \n",
    "        eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    )\n",
    "        print(\"input:  \",  pr)\n",
    "        print(\"output: \", hf_tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "        print()\n",
    "\n",
    "\n",
    "check_inference(model, TEST_PROMPTS[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aba3df",
   "metadata": {},
   "source": [
    "### Обучение модели LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd347b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback для валидации процесса обучения\n",
    "class PromptCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompts, max_new_tokens=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = prompts\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        if model is None:\n",
    "            return\n",
    "        model.eval()\n",
    "\n",
    "        for prompt in self.prompts:\n",
    "            \n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            inputs.pop(\"token_type_ids\", None)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.max_new_tokens\n",
    "                )\n",
    "\n",
    "            decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            print(\"input:  \",  prompt)\n",
    "            print(\"output: \", decoded)\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e400e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/62 : < :, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.706500</td>\n",
       "      <td>6.915845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что , что\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m      7\u001b[39m training_args = TrainingArguments(\n\u001b[32m      8\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./checkpoints\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     num_train_epochs=MAX_EPOCHS,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     report_to=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m trainer = Trainer(\n\u001b[32m     24\u001b[39m     model=model,\n\u001b[32m     25\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     ]\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/accelerator.py:2852\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2850\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# обучение модели\n",
    "\n",
    "split_dataset = dataset.select(range(2000)).train_test_split(test_size=0.01)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=16,  # 16 × 8 = 128\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"no\",\n",
    "    eval_steps=1 if DEVICE=='cpu' else 100,\n",
    "    logging_steps=1 if DEVICE=='cpu' else 100,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=hf_tokenizer,\n",
    "    callbacks=[\n",
    "        PromptCallback(\n",
    "            tokenizer=hf_tokenizer,\n",
    "            prompts=TEST_PROMPTS[:2]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True\n",
    "# )\n",
    "# model.train()\n",
    "\n",
    "# optimizer = get_optim()\n",
    "\n",
    "# total_steps = len(train_loader) * MAX_EPOCHS\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=total_steps // 10, \n",
    "#     num_training_steps=total_steps\n",
    "# )\n",
    "\n",
    "# best_loss = float(\"inf\")\n",
    "\n",
    "# for epoch in range(MAX_EPOCHS):\n",
    "\n",
    "#     progress_bar = tqdm(train_loader)\n",
    "\n",
    "#     total_loss = 0.0\n",
    "#     for step, batch in enumerate(progress_bar):\n",
    "#         batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "#         outputs = model(\n",
    "#             input_ids=batch[\"input_ids\"],\n",
    "#             attention_mask=batch[\"attention_mask\"],\n",
    "#             labels=batch[\"labels\"]\n",
    "#         )\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         progress_bar.set_description(\n",
    "#             f\"train epoch {str(epoch+1):<2} step {str(step+1):<3} loss: {loss.item():.2f}\"\n",
    "#         )\n",
    "#         loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "\n",
    "#         # Optimizer step\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         if step>0 and step % 100 == 0:\n",
    "#             progress_bar.write(f\"epoch {epoch+1} step {step+1} loss: {loss.item():.2f}\")\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "#     progress_bar.write(f\"epoch {epoch+1} finished. avg_loss: {avg_loss:.2f}\")\n",
    "    \n",
    "#     if avg_loss < best_loss:\n",
    "#         best_loss = avg_loss\n",
    "#         torch.save(model.state_dict(), SAVE_PATH)\n",
    "#         progress_bar.write(f\"best model saved with {best_loss=:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02ab7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883518e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    prompt = (\n",
    "        \"### Инструкция:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Ответ:\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model_qwen.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "for q in QUESTIONS:\n",
    "    continue\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Вопрос:\", q)\n",
    "    print(\"Ответ:\")\n",
    "    print(generate_answer(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b6b4032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20036"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad38b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-dev)",
   "language": "python",
   "name": "ml-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
