{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6bf891",
   "metadata": {},
   "source": [
    "# Проект большие языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa59ba",
   "metadata": {},
   "source": [
    "### Подготовка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61607157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast, LlamaConfig, LlamaForCausalLM, Trainer, TrainingArguments,TrainerCallback, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim import AdamW \n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e311b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4351634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE='cuda'\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"RussianNovels/corpus\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VOCAB_SIZE = 3000\n",
    "CYRILLIC_RE = re.compile(r\"^[\\p{IsCyrillic}\\s0-9.,!?—–:;\\\"'()«»…\\-]+$\")\n",
    "BATCH_SIZE = 2 if DEVICE=='cpu' else 12\n",
    "MAX_EPOCHS = 1\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP = 1.0\n",
    "SAVE_PATH = \"best_model.pt\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\"\n",
    "] \n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "QUESTIONS = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\"\n",
    "]\n",
    "\n",
    "print(f\"{DEVICE=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07dc670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RussianNovels'...\n",
      "remote: Enumerating objects: 119, done.\u001b[K\n",
      "remote: Total 119 (delta 0), reused 0 (delta 0), pack-reused 119 (from 1)\u001b[K\n",
      "Receiving objects: 100% (119/119), 21.67 MiB | 17.16 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n",
      "Cloning into 'alpaca-cleaned-ru'...\n",
      "remote: Enumerating objects: 59, done.\u001b[K\n",
      "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 59 (delta 0), reused 0 (delta 0), pack-reused 58 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (59/59), 7.81 KiB | 888.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/JoannaBy/RussianNovels.git\n",
    "!git clone https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56db64e",
   "metadata": {},
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404b8f6",
   "metadata": {},
   "source": [
    "### Подготовка корпуска текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3b3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлов в корпусе: 107\n"
     ]
    }
   ],
   "source": [
    "def collect_texts(root: Path):\n",
    "    texts = []\n",
    "    for path in root.rglob(\"*\"):\n",
    "        if path.suffix in {\".txt\"}:\n",
    "            try:\n",
    "                texts.append(path.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return texts\n",
    "\n",
    "raw_texts = collect_texts(DATA_DIR)\n",
    "raw_texts = list(set(raw_texts))\n",
    "print(f\"Файлов в корпусе: {len(raw_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c3d8316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложений после очистки: 523962\n"
     ]
    }
   ],
   "source": [
    "def filter_cyrillic_sentences(text: str):\n",
    "    '''фильтруем предложения с некириллическими символами'''\n",
    "    sentences = re.split(r\"[.!?]+\", text)\n",
    "    return [\n",
    "        s.strip()\n",
    "        for s in sentences\n",
    "        if s.strip() and CYRILLIC_RE.match(s.strip())\n",
    "    ]\n",
    "\n",
    "def normalize_punctuation(text: str) -> str:\n",
    "    '''нормализуемы пунктуацию'''\n",
    "    text = re.sub(r\"[!?]{2,}\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\.{3,}\", \"…\", text)\n",
    "    text = re.sub(r\",,{2,}\", \",\", text)\n",
    "    text = re.sub(r\"-{2,}\", \"-\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r'[«»]', '\"', text)\n",
    "    return text.strip()    \n",
    "\n",
    "clean_sentences = []\n",
    "\n",
    "for text in raw_texts:\n",
    "    sentences = filter_cyrillic_sentences(text)\n",
    "    sentences = [normalize_punctuation(s) for s in sentences]\n",
    "    clean_sentences.extend(sentences)\n",
    "\n",
    "print(f\"Предложений после очистки: {len(clean_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6095aab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка состава оставшихся спец символов\n",
      "symb | freg    | name\n",
      "' '  | 5716309 | SPACE\n",
      "','  |  853248 | COMMA\n",
      "'-'  |  279089 | HYPHEN-MINUS\n",
      "'\"'  |   49998 | QUOTATION MARK\n",
      "':'  |   44922 | COLON\n",
      "';'  |   32563 | SEMICOLON\n",
      "'–'  |   24641 | EN DASH\n",
      "')'  |    7897 | RIGHT PARENTHESIS\n",
      "'('  |    7337 | LEFT PARENTHESIS\n",
      "'—'  |    4435 | EM DASH\n",
      "'…'  |    2988 | HORIZONTAL ELLIPSIS\n",
      "'''  |    1152 | APOSTROPHE\n"
     ]
    }
   ],
   "source": [
    "print(\"Проверка состава оставшихся спец символов\")\n",
    "def is_letter_or_digit(ch: str) -> bool:\n",
    "    cat = unicodedata.category(ch)\n",
    "    return cat.startswith(\"L\") or cat.startswith(\"N\")\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for sent in clean_sentences:\n",
    "    for ch in sent:\n",
    "        if not is_letter_or_digit(ch):\n",
    "            counter[ch] += 1\n",
    "\n",
    "\n",
    "non_alnum = counter.most_common()\n",
    "print(f\"symb | freg    | name\")\n",
    "for ch, freq in non_alnum[:30]:\n",
    "    name = unicodedata.name(ch, \"UNKNOWN\")\n",
    "    print(f\"'{ch}'  | {freq:>7} | {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152d4382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чанков: 20035\n",
      "чанк  0: Я родом-то издалёка, свой край чуть помню: увезли меня оттуда по шестому году Вот только помню я длинную улицу да темный ряд избушек дымных; в конце улицы на выгоне стояли две березы тонкие - высокие Да еще помню, у нас под самым окном густые такие конопли росли, а меж коноплями тропиночка чернела, а где-то близко словно ручеек журчал, а вдали на горе лес зеленел Да еще я помню свою матушку родную Все она, бывало, в заботе, да все сиживала пригорюнившись Отца я не зазнала: он помер, мне еще и году не было Жили мы в своей избушке После, на чужой стороне, часто мне, бывало, те дни прошлые пригрезятся, что кругом поле без краю, солнце горит и жжет, сверкают серпы, валится рожь колосистая; я сижу под копной, около меня глиняный кувшинчик с водой стоит; подойдет матушка с серпом в руке, загорелая она, изморенная, напьется воды из кувшинчика, на меня глянет и мне усмехнется А зимою в печи дрова трещат, в избушке дымно; хлопочет заботная моя матушка, а в окно глянь - снежная пелена белая из глаз уходит; во всех избах сенные двери настежь, и валит из дверей дым серый Деревья стоят, инеем опушились; тихо на улице; только задорные воробьи чирикают, скачут И вдруг я в хоромах богатых очутилась, всюду шелки да бархаты, стены расписные, гвозди золоченые Стою я середь горницы замираючи, а передо мной сидит на кресле барыня молодая, пригожая, разряженная Сидела она и, глядючи на меня, усмехалась Маленькая барышня, румяненькая, кудрявая, вертелась по комнате да, смеючись, все меня беленьким пальчиком затрагивала - вот словно как деревенские ребятишки галчат дразнят Как схватили меня с улицы и посередь горницы перед барыней поставили, так и стою я да озираюсь: сердце у меня со страху закатилось Понемножку я в себя пришла и плакать стала, стала к матушке проситься Барыня в серебряный колокольчик зазвонила, и человек усатый вбежал: \"Отнеси ее домой \" - показывает ему барыня на меня, а барышня как закричит, как затопает ножками Барыня к ней целовать, унимать - барышня еще пуще\n",
      "чанк 10: За рекой, по горе отлогой, чернелись избушки, а на самой вершине реденький березовый лесочек зеленел; а там, куда ни глянь, далеко-далеко чистое поле стелется, ровное Часто, бывало, господа и наши и чужие глядят на ту гору, на избушки ветхенькие - которая покосилась, другая в землю врастает, - глядят да друг другу говорят: \"Вот, говорят, русский настоящий вид Только у нас такие виды печальные \" Чей-то барин, усатый да плечистый, все, бывало, при этом себя в грудь бил руками: \"Родное, родное \" приговариваючи Очень он яро это приговаривал Господа наши были молоды Нашу барыню все красавицей величали Такая была высокая да статная, чернобровая, белая, только ленивая Господи какая она уж ленивая-то уродилася И глянет на тебя-то вполглаза Всей работы у нее было, всего дела, что из горницы в горницу плавает, склонивши головку набок, и длинным своим платьем шелковым шуршит Оживится немножко она, разве как гости наедут, говорливые, да веселые, да осудливые Поднимут на зубки и чепчики разные и генеральшу московскую, поахают об городе Париже да побранят свой уезд, - тогда и наша барыня головку поднимет и заговорит себе громче Барин поживее ее был, веселые песенки все певал да насвистывал Говорили, что не башковит он, ну да зато смирен был; с барыней они жили согласно И она была барыня добрая Никого они не корили, не казнили, они и сердиться-то редко сердились Приди кто из людей с какой просьбою к ним - ничего, не выгонят, разве только пускать не велят, коли докучило, или пообещают, да не сделают - забудут Жили да поживали наши господа довольны да веселы, мирны да спокойны Вот это сидят, бывало, в гостиной; барин свистит, а барыня глазками по горнице поводит, и вдруг ей в голову пришло: \"Мой друг, - говорит барину, - а ведь голубые-то обои были бы лучше в гостиной \" Барин так и вскочит горошком \"Душечка какая мысль тебе хорошая пришла Где у меня-то рассудок до сих пор был \" И давай себя по лбу ляскать\n"
     ]
    }
   ],
   "source": [
    "# разбиение на чанки\n",
    "chunks = []\n",
    "current = \"\"\n",
    "\n",
    "for sent in clean_sentences:\n",
    "    max_chars = 2000  # ~512 токенов\n",
    "    if len(current) + len(sent) < max_chars:\n",
    "        current += \" \" + sent\n",
    "    else:\n",
    "        chunks.append(current.strip())\n",
    "        current = sent\n",
    "\n",
    "if current:\n",
    "    chunks.append(current.strip())\n",
    "\n",
    "print(f\"Чанков: {len(chunks)}\")\n",
    "print(f\"чанк  0: {chunks[0]}\")\n",
    "print(f\"чанк 10: {chunks[10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cb3e6",
   "metadata": {},
   "source": [
    "### Обучение токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49e3cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Нормализация текста\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "# разбиение по пробелам \n",
    "tokenizer.pre_tokenizer = Whitespace() \n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"] \n",
    "    )\n",
    "tokenizer.train_from_iterator(chunks, trainer)\n",
    "# (BOS/EOS) \n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    pair=\"[BOS] $A [EOS] $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")), \n",
    "        ], \n",
    "        )\n",
    "# Сохранение\n",
    "tokenizer.save(\"bpe_ru_3k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8d1788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', 'пример', 'те', 'к', 'ста', 'для', 'про', 'вер', 'ки', 'то', 'ке', 'ни', 'за', 'тора', '[EOS]']\n",
      "[2, 1903, 80, 31, 123, 308, 106, 170, 100, 58, 220, 70, 81, 1680, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"Пример текста для проверки токенизатора\")\n",
    "print(encoded.tokens)\n",
    "print(encoded.ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8334a89",
   "metadata": {},
   "source": [
    "### Сборка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c6c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93205f30f8244bfba757fff8ca29570d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20035 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a154e13f5ed4e658d5eb40a3503bde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20035 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"bpe_ru_3k.json\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    ")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return hf_tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "def add_labels(batch):\n",
    "    labels = []\n",
    "    for ids, mask in zip(batch[\"input_ids\"], batch[\"attention_mask\"]):\n",
    "        l = ids.copy()\n",
    "        l = [\n",
    "            token if m == 1 else -100\n",
    "            for token, m in zip(l, mask)\n",
    "        ]\n",
    "        labels.append(l)\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": chunks})\n",
    "\n",
    "dataset = dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ").map(add_labels, batched=True)\n",
    "\n",
    "dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c5204",
   "metadata": {},
   "source": [
    "### Подготовка модели LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62fe2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучаемых параметров: 128,934,912\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=2048,\n",
    "    rms_norm_eps=1e-6,\n",
    "    hidden_act=\"silu\",\n",
    "    tie_word_embeddings=True,\n",
    "    attention_bias=False,\n",
    "    rope_theta=10000.0,\n",
    "    use_cache=False,     \n",
    ")\n",
    "config.vocab_size = max(hf_tokenizer.get_vocab().values()) + 1\n",
    "config.pad_token_id = hf_tokenizer.pad_token_id\n",
    "config.bos_token_id = hf_tokenizer.bos_token_id\n",
    "config.eos_token_id = hf_tokenizer.eos_token_id\n",
    "\n",
    "model = LlamaForCausalLM(config).to(DEVICE)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Обучаемых параметров: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81028a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверим синхронизацию модели и текенизатора\n",
      "ок\n"
     ]
    }
   ],
   "source": [
    "print(\"Проверим синхронизацию модели и текенизатора\")\n",
    "assert model.config.vocab_size == max(hf_tokenizer.get_vocab().values()) + 1\n",
    "assert model.config.pad_token_id == hf_tokenizer.pad_token_id\n",
    "assert model.config.eos_token_id == hf_tokenizer.eos_token_id\n",
    "print(\"ок\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d2a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity-check начальной модели на инференсе\n",
      "при вет кры кры брат брат брат брат брат брат брат брат бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары\n"
     ]
    }
   ],
   "source": [
    "print(\"Sanity-check начальной модели на инференсе\")\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Привет\"\n",
    "inputs = hf_tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    return_token_type_ids=False\n",
    ").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=False,\n",
    "        eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(hf_tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ada5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимизатор\n",
    "def get_optim():\n",
    "    return AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optimizer = get_optim()\n",
    "\n",
    "# освобождение памяти\n",
    "def gc_collect():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b35f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сheck сходимости модели при обучении на малом датасете (2 предожения)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247c4563add7455096fc2cd044b5418b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train check:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  20 loss: 5.38\n",
      "best model saved with best_loss=5.38\n",
      "epoch:  40 loss: 4.58\n",
      "best model saved with best_loss=4.58\n",
      "epoch:  60 loss: 5.29\n",
      "epoch:  80 loss: 4.83\n",
      "epoch: 100 loss: 3.47\n",
      "best model saved with best_loss=3.47\n",
      "epoch: 120 loss: 0.72\n",
      "best model saved with best_loss=0.72\n",
      "epoch: 140 loss: 0.16\n",
      "best model saved with best_loss=0.16\n",
      "epoch: 160 loss: 0.10\n",
      "best model saved with best_loss=0.10\n",
      "epoch: 180 loss: 0.12\n",
      "epoch: 200 loss: 0.07\n",
      "best model saved with best_loss=0.07\n"
     ]
    }
   ],
   "source": [
    "print(\"Сheck сходимости модели при обучении на малом датасете (2 предожения)\")\n",
    "tiny_dataset = dataset.select(range(2))\n",
    "tiny_loader = DataLoader(tiny_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "progress_bar = tqdm(\n",
    "        range(201),\n",
    "        desc=f\"train check\",\n",
    "    )\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for step in progress_bar:\n",
    "    batch = next(iter(tiny_loader))\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "    loss = model(**batch).loss\n",
    "    progress_bar.set_description(f\"train check | loss: {loss:.2f}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step> 0 and step % 20 == 0:\n",
    "        progress_bar.write(f\"epoch: {step:>3} loss: {loss.item():.2f}\")\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "            progress_bar.write(f\"best model saved with {best_loss=:.2f}\")\n",
    "\n",
    "del tiny_dataset, tiny_loader, optimizer\n",
    "gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "614f969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сheck инференса модели после обучения на малом датасете\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия я чуть - то зли меня - - краи я чуть кая сто ка без краи и я краи году из бу ных но ро меня , у нас у - то я тон тро тся , мне тем но ро самы кие помню , в ш под кие ле\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха краи сто - то ро , а что кругом часто где жно к двери потом же т и же т и \", а я ок я не было я ли - я рин и вдруг я шке ; чь еи я самы без ни чу т , да еще я\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Сheck инференса модели после обучения на малом датасете\")\n",
    "def check_inference(model, prompts: list):\n",
    "    model.eval()\n",
    "    for pr in prompts:\n",
    "        inputs = hf_tokenizer(pr, return_tensors=\"pt\").to(model.device)\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "        out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True, \n",
    "        eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    )\n",
    "        print(\"input:  \",  pr)\n",
    "        print(\"output: \", hf_tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "        print()\n",
    "\n",
    "\n",
    "check_inference(model, TEST_PROMPTS[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aba3df",
   "metadata": {},
   "source": [
    "### Обучение модели LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd347b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback для валидации процесса обучения\n",
    "class PromptCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompts, max_new_tokens=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = prompts\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def _one_answer(self, model, prompt):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                do_sample=True, \n",
    "                eos_token_id=hf_tokenizer.eos_token_id,\n",
    "            )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        if model is None:\n",
    "            return\n",
    "        model.eval()\n",
    "        print(\"=\"*20, f\"on step {state.global_step}\", \"=\"*20)\n",
    "        for prompt in self.prompts[:3]:\n",
    "            decoded = self._one_answer(model, prompt)\n",
    "            print(\"input:  \",  prompt)\n",
    "            print(\"output: \", decoded)\n",
    "            print()    \n",
    "            \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        if model is None:\n",
    "            return\n",
    "        model.eval()\n",
    "        print(\"=\"*20, f\"ON TRAIN END\", \"=\"*20)        \n",
    "        for prompt in self.prompts:\n",
    "            decoded = self._one_answer(model, prompt)\n",
    "            print(\"input:  \",  prompt)\n",
    "            print(\"output: \", decoded)\n",
    "            print()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e400e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.014700</td>\n",
       "      <td>6.057720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.915700</td>\n",
       "      <td>5.911452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.837300</td>\n",
       "      <td>5.875160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== on step 2 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия ли ре - на сте чи - не мог , - у пря т я - не уме л ясь и , от тя тив р ет м х х та и - и , да - сказал он к - при ду в свою ру , - на ле\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха ло , с ним ко сти м ; в сто ет , к двери , ка в нем с про сты лись - с , я ме ты ; - на него - а он - то и - то , но ну , от сюда , о пе чка\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния ло чь , как - с ним ко ря л а ри нь ясь , рас серди сь , при шла ю \" я в лицо , - ка , но вои ле ва л у стави шь ли , а на пе жали , не уда рить , он\n",
      "\n",
      "==================== on step 4 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия на гра чу с виде т ри ка ми не мог пу к нему его ее , и все , он под бе до х , как будто , на сту ка с нею вы ки не мог быть и у нее на нее , но по про ле ,\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха из ки под тя не было по ряд по смотри д ва , под ъез ти ние с дви г ка ко ва ку ли за ставля ю : \" и по стави ть , об пе и у лицы \", - при бе ч нее в памя т ;\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния ни к нув не могу , а было ли и с неи я , как будто под тя го га , да и со би на сте ве ва ; из я же не могу : \" от пусти ли ее и раз при ве ра ма р ка в\n",
      "\n",
      "==================== on step 6 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия , которые - а - таки на неи , как , - он был о в окно , у бе х , - что от нее и , - со бы на шла она , что он рас серди ться до шла , - да к столу , так ,\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха на ко ро сло ть , по у ши те ны , точно быть : - а она не было , и по мя не да , - спросил : там а он ни ма ку - от мы к ка ют , как не от ку па нь ;\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния не сме ту , она в памя , что а было у меня но , со шла в котором не вид ала , что в сторону к столу , у лицу : - на сла не уда дь - с бо вала , а вы , и что у твер\n",
      "\n",
      "==================== on train end ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия на него за кри ками , то чка ; со се ла на пе ль ле м было все при хо ну , рас поло ве , под - не слы в у бор нее , вы ш ше н на нем , с кем вы жи х , что\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха ну - к ва , - в ма \" - от бе х и по му , на у х ная на сту и за кри чала к на строе ние на , до пы , все по - с вами \" по прави на , с жа на улице\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния у твер тил с жи , и то в этом ма ти вая во ши ться , - да и я , он , со чи н т , - , об да и со жале лась , которые \" к ок ни в у лицу ли га ми ,\n",
      "\n",
      "input:   Человек сознает себя свободным\n",
      "output:  человек со знает себя свобо д ным за , при дет по нимал по - вот как - он , и не так у пи ла - то ле жали , а в городе от до ба ала с ним , и не сме ш ался с ходи ть - то , то - до х \"\n",
      "\n",
      "input:   Что бы ни случилось, я всегда буду\n",
      "output:  что бы ни случилось , я всегда буду , а на улице за шла я был и еще на помни , но , к себе себя ; он от до носи , он ; я думаю , в у бе чка , на него х и от ъез ром , не прият ные , в нем , к\n",
      "\n",
      "input:   Любовь мешает смерти\n",
      "output:  любовь ме ша ет смерти и при него на на ки у мо сы м , от по нимал , чтоб от нее , когда что он на голове , которыи , что он не бу ч и об на другои день , по улице , чтоб и со шла о су н , что\n",
      "\n",
      "input:   Нет, жизнь не кончена\n",
      "output:  нет , жизнь не кон че на шку , ми и при шла к чему , с верх - не могу в углу фе ныи , как не по нимал , а он в другои су х , что ж от него ; они при жи ется за бот , она еще за ма ра ко и\n",
      "\n",
      "input:   Всякая мысль, даже самая простая\n",
      "output:  вся кая мысль , даже сама я про стая , и ро х ее , но только как не так было же у лицы - при казы вать , а она - то в на гра ле ча , рас сы , как будто , рас мы , как на другои , он , он на груди ; пре\n",
      "\n",
      "input:   Война не любезность, а самое гадкое дело\n",
      "output:  вои на не любез но сть , а самое га д кое дело м , ма ру , а га м : - и в се ти , а он вы дума в конце у хо , она на нем о забо жи лась \" не поня ть на , но : \" за гляды ть от нее , я под тя не\n",
      "\n",
      "input:   Чтобы жить честно\n",
      "output:  чтобы жить че стно за бе го ро и его с ли было вы шло ку де ла к с него , ни не от стави лась от того , чтоб не про ле жали ее , так она , - то при кры т , он - не о гра ду к тому\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обучение модели\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.01)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=8,  # 8 × 12 = 96\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"no\",\n",
    "    eval_steps=1 if DEVICE=='cpu' else 50,\n",
    "    logging_steps=1 if DEVICE=='cpu' else 50,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=hf_tokenizer,\n",
    "    callbacks=[\n",
    "        PromptCallback(\n",
    "            tokenizer=hf_tokenizer,\n",
    "            prompts=TEST_PROMPTS\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02ab7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae173c",
   "metadata": {},
   "source": [
    "## Post-train SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b40e1",
   "metadata": {},
   "source": [
    "### Подготовка модели QWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9bb76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_qwen = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, use_fast=True\n",
    ")\n",
    "\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e41b49",
   "metadata": {},
   "source": [
    "### Ответы сырой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883518e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input 1:\n",
      "сколько планет в нашей солнечной системе?\n",
      "Model Output 1:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "сколько планет в нашей солнечной системе?\n",
      "moire\n",
      "некоторое количество планет, которые могут быть существующими во сне, включают:\n",
      "moire\n",
      "недомыслие\n",
      "moire\n",
      "населиться\n",
      "moire\n",
      "пермата\n",
      "moire\n",
      "мутного\n",
      "========================================\n",
      "Model Input 2:\n",
      "расскажи стих\n",
      "Model Output 2:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "расскажи стих\n",
      "ogenee eka, dhošte, eka, rasa, neshka. \n",
      "akra, eka, eka, neshka. \n",
      "akra, eka, eka, eka, neshka. \n",
      "\n",
      "========================================\n",
      "Model Input 3:\n",
      "когда собирать крыжовник?\n",
      "Model Output 3:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "когда собирать крыжовник?\n",
      " libertine\n",
      "\n",
      "ocodersystem\n",
      "You are a helpful assistant.웬\n",
      "웬user\n",
      "Когда пользоваться «Затушиться» можно?웬\n",
      "ocodersystem\n",
      "You are a helpful assistant.웬\n",
      "웬user\n",
      "что нужно для зак\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "def check_outputs(model, tokenizer, questtions):\n",
    "    def _generate_answer(question):\n",
    "        mess = [\n",
    "            {\"role\": \"user\",\n",
    "            \"content\": question}\n",
    "            ]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            mess,\n",
    "            return_tensors=\"pt\",\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        ).to(model_qwen.device)\n",
    "\n",
    "        output = model_qwen.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "    for idx, q in enumerate(questtions):\n",
    "        print(f\"Model Input {idx+1}:\")\n",
    "        print(q)\n",
    "        print(f\"Model Output {idx+1}:\")\n",
    "        print(_generate_answer(q))\n",
    "        print(\"=\"*40)\n",
    "\n",
    "check_outputs(model_qwen, tokenizer_qwen, QUESTIONS[:3])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f759fa",
   "metadata": {},
   "source": [
    "### Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b637496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51760"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"d0rj/alpaca-cleaned-ru\", split=\"train\")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1df52a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nДайте три совета, как оставаться здоровым.<|im_end|>\\n<|im_start|>assistant\\n1. Соблюдайте сбалансированную и питательную диету. Убедитесь, что в ваш рацион входят разнообразные фрукты и овощи, нежирный белок, цельнозерновые продукты и полезные жиры. Это помогает обеспечить ваш организм необходимыми питательными веществами для оптимального функционирования и может помочь предотвратить хронические заболевания.\\n\\n2. Занимайтесь регулярной физической активностью. Упражнения имеют решающее значение для поддержания крепких костей, мышц и здоровья сердечно-сосудистой системы. Старайтесь уделять не менее 150 минут умеренным аэробным упражнениям или 75 минут интенсивным упражнениям каждую неделю.\\n\\n3. Высыпайтесь. Достаточное количество качественного сна имеет решающее значение для физического и психического благополучия. Он помогает регулировать настроение, улучшать когнитивные функции и поддерживает здоровый рост и иммунную функцию. Старайтесь спать 7-9 часов каждую ночь.<|im_end|>\\n'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_to_message(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {'role': 'user', 'content': example['instruction']},\n",
    "            {'role': 'assistant', 'content': example['output']}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "ds_chat = ds.map(\n",
    "    example_to_message,\n",
    "    batched=False,\n",
    "    remove_columns=['input', 'instruction', 'output', ],\n",
    ").map(\n",
    "    lambda x: {\n",
    "        'text': tokenizer_qwen.apply_chat_template(x['messages'],\n",
    "         tokenize=False)\n",
    "         },\n",
    "        remove_columns=['messages'],\n",
    "    )\n",
    "ds_chat[0]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e65e3",
   "metadata": {},
   "source": [
    "### Обучение модели QWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bfea8d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() missing 1 required positional argument: 'output_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     12\u001b[0m     model_qwen,\n\u001b[1;32m     13\u001b[0m     args\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     14\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mds_chat,\n\u001b[1;32m     15\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mtokenizer_qwen,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain() \n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() missing 1 required positional argument: 'output_dir'"
     ]
    }
   ],
   "source": [
    "config = TrainingArguments(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    report_to='none',\n",
    "    logging_steps=1 if DEVICE=='cpu' else 50,\n",
    "    save_strategy='no',\n",
    "    gradient_accumulation_steps=8\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model_qwen,\n",
    "    args=config,\n",
    "    train_dataset=ds_chat,\n",
    "    processing_class=tokenizer_qwen,\n",
    ")\n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17f68a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
