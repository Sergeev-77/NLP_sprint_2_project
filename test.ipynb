{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6bf891",
   "metadata": {},
   "source": [
    "# Проект большие языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa59ba",
   "metadata": {},
   "source": [
    "### Подготовка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "61607157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast, LlamaConfig, LlamaForCausalLM, Trainer, TrainingArguments,TrainerCallback, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim import AdamW \n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from trl import SFTTrainer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e311b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# освобождение памяти\n",
    "def gc_collect():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize() \n",
    "\n",
    "gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4351634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE='cuda'\n",
      "NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"RussianNovels/corpus\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VOCAB_SIZE = 3000\n",
    "CYRILLIC_RE = re.compile(r\"^[\\p{IsCyrillic}\\s0-9.,!?—–:;\\\"'()«»…\\-]+$\")\n",
    "BATCH_SIZE = 2 if DEVICE=='cpu' else 128\n",
    "MAX_EPOCHS = 12\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP = 1.0\n",
    "SAVE_PATH = \"best_model.pt\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\"\n",
    "] \n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "QUESTIONS = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\"\n",
    "]\n",
    "\n",
    "print(f\"{DEVICE=}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07dc670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'RussianNovels' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'alpaca-cleaned-ru' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/JoannaBy/RussianNovels.git\n",
    "!git clone https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56db64e",
   "metadata": {},
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404b8f6",
   "metadata": {},
   "source": [
    "### Подготовка корпуска текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c3b3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлов в корпусе: 107\n"
     ]
    }
   ],
   "source": [
    "def collect_texts(root: Path):\n",
    "    texts = []\n",
    "    for path in root.rglob(\"*\"):\n",
    "        if path.suffix in {\".txt\"}:\n",
    "            try:\n",
    "                texts.append(path.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return texts\n",
    "\n",
    "raw_texts = collect_texts(DATA_DIR)\n",
    "raw_texts = list(set(raw_texts))\n",
    "print(f\"Файлов в корпусе: {len(raw_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c3d8316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложений после очистки: 523962\n"
     ]
    }
   ],
   "source": [
    "def filter_cyrillic_sentences(text: str):\n",
    "    '''фильтруем предложения с некириллическими символами'''\n",
    "    sentences = re.split(r\"[.!?]+\", text)\n",
    "    return [\n",
    "        s.strip()\n",
    "        for s in sentences\n",
    "        if s.strip() and CYRILLIC_RE.match(s.strip())\n",
    "    ]\n",
    "\n",
    "def normalize_punctuation(text: str) -> str:\n",
    "    '''нормализуемы пунктуацию'''\n",
    "    text = re.sub(r\"[!?]{2,}\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\.{3,}\", \"…\", text)\n",
    "    text = re.sub(r\",,{2,}\", \",\", text)\n",
    "    text = re.sub(r\"-{2,}\", \"-\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r'[«»]', '\"', text)\n",
    "    return text.strip()    \n",
    "\n",
    "clean_sentences = []\n",
    "\n",
    "for text in raw_texts:\n",
    "    sentences = filter_cyrillic_sentences(text)\n",
    "    sentences = [normalize_punctuation(s) for s in sentences]\n",
    "    clean_sentences.extend(sentences)\n",
    "\n",
    "print(f\"Предложений после очистки: {len(clean_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6095aab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка состава оставшихся спец символов\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symb | freg    | name\n",
      "' '  | 5716309 | SPACE\n",
      "','  |  853248 | COMMA\n",
      "'-'  |  279089 | HYPHEN-MINUS\n",
      "'\"'  |   49998 | QUOTATION MARK\n",
      "':'  |   44922 | COLON\n",
      "';'  |   32563 | SEMICOLON\n",
      "'–'  |   24641 | EN DASH\n",
      "')'  |    7897 | RIGHT PARENTHESIS\n",
      "'('  |    7337 | LEFT PARENTHESIS\n",
      "'—'  |    4435 | EM DASH\n",
      "'…'  |    2988 | HORIZONTAL ELLIPSIS\n",
      "'''  |    1152 | APOSTROPHE\n"
     ]
    }
   ],
   "source": [
    "print(\"Проверка состава оставшихся спец символов\")\n",
    "def is_letter_or_digit(ch: str) -> bool:\n",
    "    cat = unicodedata.category(ch)\n",
    "    return cat.startswith(\"L\") or cat.startswith(\"N\")\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for sent in clean_sentences:\n",
    "    for ch in sent:\n",
    "        if not is_letter_or_digit(ch):\n",
    "            counter[ch] += 1\n",
    "\n",
    "\n",
    "non_alnum = counter.most_common()\n",
    "print(f\"symb | freg    | name\")\n",
    "for ch, freq in non_alnum[:30]:\n",
    "    name = unicodedata.name(ch, \"UNKNOWN\")\n",
    "    print(f\"'{ch}'  | {freq:>7} | {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "152d4382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чанков: 20041\n",
      "чанк  0: Навстречу ко мне выбежал слуга Я потребовал комнату и прибавил: - Смотри же, только чистую, пожалуйста - Уж не побрезгайте, окнами на двор,- умильно глядя на меня, сказал слуга Я заглянул на грязный двор, заставленный различными весьма странными экипажами Под навесом стояли лошади, коровы, бараны На дворе толпились мужики; шум был ужасный Желая хорошенько выспаться после трех ночей, проведенных в телеге, я потребовал комнату непременно с окнами на улицу - Все занято,- отвечал слуга - А налево-то от нас, Архип - раздался мужской голос над нашими головами В окне второго этажа покоились животами на пуховых подушках в ситцевых наволочках старик и старушка - Занято - нехотя отвечал Архип на их замечание - Ну так пятый номер, что опорожнил сегодня купец,- подхватила старушка - Исправник взял под кого-то - грубо крикнул Архип Благодаря заботливости стариков, мне ничего более не оставалось, как попытать счастья в другом трактире - Ты поезжай к немцу, может, у него есть - заметил старичок моему ямщику - Под гору не езди, а ступай низом - тут ближе: я хаживала пешком,- с горячностью прибавила старушка Ямщик тронулся, я поклонился старичкам, благодаря за непрошеные услуги Они отвечали самыми радушными поклонами По случаю ярмарки даже все харчевни городишка были битком набиты, и я скоро принужден был возвратиться к первому трактиру Старички, завидев меня, раскланялись со мной уже как с коротким знакомым и с участием спросили: нашел ли я номер - Нет - отвечал я, выходя из телеги, и обратился к выбежавшему Архипу: - Давай хоть на двор номер, что делать Архип торжественно отвечал: - Да и его сейчас заняли Это известие меня ошеломило - Ишь какой, мы ведь тебе сказывали: обожди Барин, может статься, и вернется,- строго заметила ему старушка и, обратись ко мне, продолжала с чувством обиженного достоинства: - В ярмарку дворянам здесь места нет: купечество все захватывает, хоть на улице ночуй Это меня нисколько не утешило, я пристал к Архипу, чтоб давал мне номер\n",
      "чанк 10: Итак, я оставил моего приятеля самым отчаянным мизантропом и теперь соображал, какое превращение должна была совершить с ним уединенная деревенская жизнь Он, вероятно, похудел, одичал; чуждается людей, постоянно молчит и бродит по лесам, оплакивая слабости человечества Ямщик объявил, что скоро должно показаться Уткино Дорога пошла между полями и такая узкая, что канавы, выкопанные по бокам, препятствовали разъехаться двум встречным А между тем навстречу моего экипажа ехали беговые дрожки; ими правил какой-то господин, а сзади его сидел кучер и басил: - Правей, правей Мой ямщик, оглядясь на обе стороны, сердито крикнул: - Куда правей Не видишь, что ль, канава Ну, держи сам правей И он придержал телегу Беговые дрожки подъехали близко, кучер продолжал кричать: - Правей - Ну что же у вас будет - сказал я - Да не валить же вашу милость в канаву Проезжай - сказал сердито мой ямщик Тогда сам барин крикнул: - Держи левей Беговые дрожки едва могли проехать шагом мимо меня Я имел время рассмотреть господина, правившего чуть ли не столетним рысаком Рост его был средний, но полнота скрадывала его Полная его фигура была облечена в белый парусиновый балахон в виде пальто-сак и из той же материи широкие панталоны На голове пестрый картуз затейливого фасона Полные его щеки и двойной подбородок слегка обросли бурыми иглами, а усы были желтовато-пепельные Господин в балахоне, страшась, верно, очутиться в канаве, все внимание свое обратил на вожжи - Уткинский барин - сказал мне ямщик - Как Не может быть Иван Андреич - воскликнул я - Да-с, они-с Я не признал их сначала - Стой, стой - закричал я, повернувшись назад и махая руками к дрожкам Чему я так обрадовался, сам не знаю Впрочем, в юности дружба так пылка и снисходительна, так проста и горяча, что нет места в уме для анализа Любишь человека сам не знаешь за что; иногда по привычке Как часто я видел ужасно суровых стариков, делавшихся мягкими и веселыми при одном воспоминании молодости\n"
     ]
    }
   ],
   "source": [
    "# разбиение на чанки\n",
    "chunks = []\n",
    "current = \"\"\n",
    "\n",
    "for sent in clean_sentences:\n",
    "    max_chars = 2000  # ~512 токенов\n",
    "    if len(current) + len(sent) < max_chars:\n",
    "        current += \" \" + sent\n",
    "    else:\n",
    "        chunks.append(current.strip())\n",
    "        current = sent\n",
    "\n",
    "if current:\n",
    "    chunks.append(current.strip())\n",
    "\n",
    "print(f\"Чанков: {len(chunks)}\")\n",
    "print(f\"чанк  0: {chunks[0]}\")\n",
    "print(f\"чанк 10: {chunks[10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cb3e6",
   "metadata": {},
   "source": [
    "### Обучение токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d49e3cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Нормализация текста\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "# разбиение по пробелам \n",
    "tokenizer.pre_tokenizer = Whitespace() \n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"] \n",
    "    )\n",
    "tokenizer.train_from_iterator(chunks, trainer)\n",
    "# (BOS/EOS) \n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    pair=\"[BOS] $A [EOS] $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")), \n",
    "        ], \n",
    "        )\n",
    "# Сохранение\n",
    "tokenizer.save(\"bpe_ru_3k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d8d1788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', 'пример', 'те', 'к', 'ста', 'для', 'про', 'вер', 'ки', 'то', 'ке', 'ни', 'за', 'тора', '[EOS]']\n",
      "[2, 1903, 80, 31, 123, 308, 106, 170, 100, 58, 220, 70, 81, 1680, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"Пример текста для проверки токенизатора\")\n",
    "print(encoded.tokens)\n",
    "print(encoded.ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8334a89",
   "metadata": {},
   "source": [
    "### Сборка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7c6c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d94ecf1d8094959b02d26ac055dc27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46225813efe42fdbc1b68a6534b4825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"bpe_ru_3k.json\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    ")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return hf_tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "def add_labels(batch):\n",
    "    labels = []\n",
    "    for ids, mask in zip(batch[\"input_ids\"], batch[\"attention_mask\"]):\n",
    "        l = ids.copy()\n",
    "        l = [\n",
    "            token if m == 1 else -100\n",
    "            for token, m in zip(l, mask)\n",
    "        ]\n",
    "        labels.append(l)\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": chunks})\n",
    "\n",
    "dataset = dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ").map(add_labels, batched=True)\n",
    "\n",
    "dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c5204",
   "metadata": {},
   "source": [
    "### Подготовка модели LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62fe2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучаемых параметров: 128,934,912\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=2048,\n",
    "    rms_norm_eps=1e-6,\n",
    "    hidden_act=\"silu\",\n",
    "    tie_word_embeddings=True,\n",
    "    attention_bias=False,\n",
    "    rope_theta=10000.0,\n",
    "    use_cache=False,     \n",
    ")\n",
    "config.vocab_size = max(hf_tokenizer.get_vocab().values()) + 1\n",
    "config.pad_token_id = hf_tokenizer.pad_token_id\n",
    "config.bos_token_id = hf_tokenizer.bos_token_id\n",
    "config.eos_token_id = hf_tokenizer.eos_token_id\n",
    "\n",
    "model = LlamaForCausalLM(config).to(DEVICE)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Обучаемых параметров: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81028a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверим синхронизацию модели и текенизатора\n",
      "ок\n"
     ]
    }
   ],
   "source": [
    "print(\"Проверим синхронизацию модели и текенизатора\")\n",
    "assert model.config.vocab_size == max(hf_tokenizer.get_vocab().values()) + 1\n",
    "assert model.config.pad_token_id == hf_tokenizer.pad_token_id\n",
    "assert model.config.eos_token_id == hf_tokenizer.eos_token_id\n",
    "print(\"ок\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10d2a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity-check начальной модели на инференсе\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "при вет кры кры брат брат брат брат брат брат брат брат бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары\n"
     ]
    }
   ],
   "source": [
    "print(\"Sanity-check начальной модели на инференсе\")\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Привет\"\n",
    "inputs = hf_tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    return_token_type_ids=False\n",
    ").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=False,\n",
    "        eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(hf_tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ada5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимизатор\n",
    "def get_optim(model):\n",
    "    return AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optimizer = get_optim(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b35f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сheck сходимости модели при обучении на малом датасете (2 предожения)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ede3ecee364d45a375b59b2f9db112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train check:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  20 loss: 5.70\n",
      "best model saved with best_loss=5.70\n",
      "epoch:  40 loss: 5.16\n",
      "best model saved with best_loss=5.16\n",
      "epoch:  60 loss: 5.05\n",
      "best model saved with best_loss=5.05\n",
      "epoch:  80 loss: 2.97\n",
      "best model saved with best_loss=2.97\n",
      "epoch: 100 loss: 0.50\n",
      "best model saved with best_loss=0.50\n",
      "epoch: 120 loss: 0.04\n",
      "best model saved with best_loss=0.04\n",
      "epoch: 140 loss: 0.01\n",
      "best model saved with best_loss=0.01\n",
      "epoch: 160 loss: 0.00\n",
      "best model saved with best_loss=0.00\n",
      "epoch: 180 loss: 0.00\n",
      "best model saved with best_loss=0.00\n",
      "epoch: 200 loss: 0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"Сheck сходимости модели при обучении на малом датасете (2 предожения)\")\n",
    "tiny_dataset = dataset.select(range(2))\n",
    "tiny_loader = DataLoader(tiny_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "progress_bar = tqdm(\n",
    "        range(201),\n",
    "        desc=f\"train check\",\n",
    "    )\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for step in progress_bar:\n",
    "    batch = next(iter(tiny_loader))\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "    loss = model(**batch).loss\n",
    "    progress_bar.set_description(f\"train check | loss: {loss:.2f}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step> 0 and step % 20 == 0:\n",
    "        progress_bar.write(f\"epoch: {step:>3} loss: {loss.item():.2f}\")\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "            progress_bar.write(f\"best model saved with {best_loss=:.2f}\")\n",
    "\n",
    "del tiny_dataset, tiny_loader, optimizer\n",
    "gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "614f969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сheck инференса модели после обучения на малом датасете\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия на скои па вече вы объя вы встречу меня , да ,- да и то на двор , не по знает я делать , да меня , не муж вы знает слу ге : - уж не зная , ар хи п , зя , не отвечал ар хи п\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха ар хи п на от рез объя вил , что нет ,- разве не вы будет ли кто к вече ру , да и то бог знает я стоял в недо уме нии , не зная , что делать , как вдруг стари чок крикнул слу ге : - ар\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Сheck инференса модели после обучения на малом датасете\")\n",
    "def check_inference(model, prompts: list):\n",
    "    model.eval()\n",
    "    for pr in prompts:\n",
    "        inputs = hf_tokenizer(pr, return_tensors=\"pt\").to(model.device)\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "        out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True, \n",
    "        eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    )\n",
    "        print(\"input:  \",  pr)\n",
    "        print(\"output: \", hf_tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "        print()\n",
    "\n",
    "\n",
    "check_inference(model, TEST_PROMPTS[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aba3df",
   "metadata": {},
   "source": [
    "### Обучение модели LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd347b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback для валидации процесса обучения\n",
    "class PromptCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompts, max_new_tokens=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = prompts\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def _one_answer(self, model, prompt):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                do_sample=True, \n",
    "                eos_token_id=hf_tokenizer.eos_token_id,\n",
    "            )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        if model is None:\n",
    "            return\n",
    "        model.eval()\n",
    "        print(\"=\"*20, f\"on step {state.global_step}\", \"=\"*20)\n",
    "        for prompt in self.prompts[:3]:\n",
    "            decoded = self._one_answer(model, prompt)\n",
    "            print(\"input:  \",  prompt)\n",
    "            print(\"output: \", decoded)\n",
    "            print()    \n",
    "            \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        if model is None:\n",
    "            return\n",
    "        model.eval()\n",
    "        print(\"=\"*20, f\"ON TRAIN END\", \"=\"*20)        \n",
    "        for prompt in self.prompts:\n",
    "            decoded = self._one_answer(model, prompt)\n",
    "            print(\"input:  \",  prompt)\n",
    "            print(\"output: \", decoded)\n",
    "            print()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e400e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 27:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.428000</td>\n",
       "      <td>5.102242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.841100</td>\n",
       "      <td>4.618660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.387800</td>\n",
       "      <td>4.284968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.075100</td>\n",
       "      <td>4.125978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>4.043071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.664600</td>\n",
       "      <td>4.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.498600</td>\n",
       "      <td>4.021312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.322900</td>\n",
       "      <td>4.087390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.195300</td>\n",
       "      <td>4.124145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== on step 50 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия х , все более вели чи ми ся и даже с тобои от стра ст нои , чтобы с нима ть , которыи в этом отноше ниях в самом деле что и то чка , да ст рые , к ли ще нием и без сомне ния , на ле\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха : он только ты не види м их , раз би р ались - по тря ше и - про па кли - с , я знаю ты вать , - сказал ему , нет , пере бира ющии тебя , при ну че ли на , о хва тил\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния не могла про шеп тал с собои , но он не только в эту минуту не при этом ; но если б рю я в чем время не которые , не под ходил до этого у вели чи ка , а там да ве , не которые по красне ли\n",
      "\n",
      "==================== on step 100 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия на гра де с виде тельство м с ми ром ского пу бли оте чи ю , и все это было бы трудно : \" ма кар , да мы будем делать с нею вы слу ха ете но ша ете не на меня , но по про ле та\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха из за го та нного те ту ф ли не за гра фа та тами от дель ного а ри ка ти в се ли вои ны не вы ходил из по ток и за слу жи вшеи ся пе сни ца м - р э и , а по\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния сть к нему не до би т , ли ше но же я , с тех самы ми го нь ко о се ни не на то , что он всегда в душе ему из воли л от своего месте , и я не только не мог раз да вать\n",
      "\n",
      "==================== on step 150 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия ра ская ния , которые с ны вали про да рь не было со страда ние , и теперь , хотя , не име вшие даже и не было в обще и не поня тно , в москве , при нимал все , что это могло было , и все\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха на ко жа , что не по у ши те ему не от лично е и бе се да че та сти ческои це ло еи , которая не знала , а теперь она ничего еще при ним ала к ка вале ру , не от рез ала , но\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния не , от вернулся к неи я , не думал было ни в но мере со бы тия ; не только не пере да нныи стра х , ни же не нь кии , и сам должен был по казать ему как об этом вы бра ня ть у меня\n",
      "\n",
      "==================== on step 200 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия ра ев все это о хва ты ва ешь , на си лу и раз ъя сни л все это , - о , ми лая , под би тое и не у серди вшее , и он с пре зре нием пре с о ка жи сь с тем\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха за бу к ва ты , хотя он уже о ча рова тельно и по нимал , что у стра и вать о ра то рии - с ; да он не может до гна ть все по - преж нему \" по прави лось , но он не мог\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния у твер ждал этого не за ра то сть и что до этого не будет , так - говорил ему : \" ты , со чи не шься , - что с ними и со жале ю но ты \" - \" как ты говори шь - да , что\n",
      "\n",
      "==================== on step 250 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия сть , для того что же по ставле нных в сра вне нии с не которым пре зре ниями - в том , чтобы жить в городе и до ставля ет нам , а и нои человек , в с мя те нии , на то же время и потому\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха , а то ли хо ди ны , и пере кре сти лся в бли жаи ших случа ях , чтобы про честь , все ли равно ; но в не бе си мую на себе пу ста ти , и , конечно , и при этом за бот к\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния и при чина , что он теперь будет под да ться от него , не рас стоя тся , когда теперь он любил , но что это может только для нас нет и ему теперь нельзя оста вить и что вы были с виде тели теперь же вы не бу\n",
      "\n",
      "==================== on step 300 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия о за ря ды , я не могу пред ставить ее - не могу быть ва шим все это - не так , как он говорит , - отвечала мать - вот видите как бог дал вот тебе слово , за что я говорю - сказал клим , нах му\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха , и , по гло тив , что от не сли сь к у бь юще му за тру дня нию к но ге в даль нюю комнату , раз ор ва в ее и , рас кла ня вшись на гу ль нов , начал пере бира ть свои\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния м , для чего он сделал себе это последнее время и в ту ночь , как он вы держал в себе и , когда все на нем нет ни на что , то и ни на что не в сила х была ему от ло жить в этом ( по\n",
      "\n",
      "==================== on step 350 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия не могло быть со сре до то че ны , а потому , что даже с мы ш ле ют ка мни , от ка жу т от про ле та рия , так что в обще м состоя нии были пре вра тки в их памя ть но из\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха ра кли у нее , с я б ло ками , чер ным ко пыт ником в зад и вперед все время в з але про бе гали до под лу из ка , не вы держа в от сут ствие , не заметно , не ше ве ля сь\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния о том , что он даже о пыт но , когда он по гу бил ее , - на еди не он про бежал с ним она вы нула из себя за у гла , за гля нула на него и по нес ла на у хо , - но\n",
      "\n",
      "==================== on step 400 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия для нас , не было из вле ка нии по сред ст в по че сть ку ми ра , под ли нного и не раз ры ва но на па с ше сть десят лет , по у гла м , на по чи нные про ви ан ту\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха при обре сти из - за по ряд ка , вы ходя щих на верх ние у лицы , из которых вы скочи ли после , чтобы раз носи ть ро ж д сто ников , по к ров , с при це р чи ва ющих ся всех бы\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния бы на смеш ливо е , не же ли ему пред ме том себе , но по скорее принима я его за бот ы о со ста ве , о том , как он по ко си лся на его са р та ну , при шед шего не заме\n",
      "\n",
      "==================== on step 450 ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия для при ли чи я , я очень не могу у жи на ть , чтобы при нем со образи в , но вы сказ али бы это из воли ли быть еще более или ме нее при всех своих со страда нии не могут вы по говори м ,\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха на вы ступи л за до мом он с ду ру при сло ня ет у сы на спи не спи чка : в углу бле сте ли у са дь бу с ба са в ски ну т ал на них - откуда вы так ра но за ставля\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния не только о другом , но и только вместе без при т чи ве е , без це ре мо нь , и дя дя его ; он даже , за бе гая к само му себе перед собою , и вот теперь , только тогда , когда говорил себе\n",
      "\n",
      "==================== ON TRAIN END ====================\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия в россии , из му че нное ува жение , почти и не прия з не нное отноше ние к обще ству людеи и деи , об ъез жая вы хода в и по гре бе ку вод ки , до шла ба пи т вы ли ня н чи\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха раз ло жения и со зда вания в слу шно рас про стра ня вшихся , от которых он не имел в ка ю но ша с видом , что было очень просто , просто и легко мысле нно он был так , как видел , что для себя ,\n",
      "\n",
      "input:   Мысль о том, что он принес страдания\n",
      "output:  мысль о том , что он при нес страда ния жи телеи у пря мым , хотя в городе уже в пол ном ли тера ту ре его при н ци пе не во леи , мы у него и не сли сь ноги , при сло ня сь к его ще ке , а у него при сло ни\n",
      "\n",
      "input:   Человек сознает себя свободным\n",
      "output:  человек со знает себя свобо д ным им а б а ф аль нце ву - он по пал без у словно , когда мы говорили , когда тот стоял на ка том месте , и сказал : \" ду хов ныи сын \" а на другои день я стал и скать су мо чку на ве\n",
      "\n",
      "input:   Что бы ни случилось, я всегда буду\n",
      "output:  что бы ни случилось , я всегда буду только так , как они из по мы ш ле ются , так , ка зна ках , что не на ходит и не по - ново му , не без о снова ния , ибо хотя т их по ступ рен им по ш лю т в ви ц\n",
      "\n",
      "input:   Любовь мешает смерти\n",
      "output:  любовь ме ша ет смерти и о сла бе л кои и любви , которая не могла наи ти в неи от у мы вания и того и на чать и ма мои , чтобы не было сказа но и от клады вать и от зы вается в нем в глазах , для которои ,\n",
      "\n",
      "input:   Нет, жизнь не кончена\n",
      "output:  нет , жизнь не кон че на , конечно , вам на верно , что все это только зло вное на писа нное – с бо ра то ром и при вели , да все не то , – да , вы знаете , – о нет , я понимаю , кого они ну жны , то\n",
      "\n",
      "input:   Всякая мысль, даже самая простая\n",
      "output:  вся кая мысль , даже сама я про стая с пала на пере ки ну тую голову ; и потом над неи вз али ее , и она при жим ала их к себе и шею , и слезы за сти л ала их под по ду шки и за мя ться еи на руку , чтоб на ее\n",
      "\n",
      "input:   Война не любезность, а самое гадкое дело\n",
      "output:  вои на не любез но сть , а самое га д кое дело не очень много и так , особенно , не много таких не прия тель , столь не ви да вшии их , как с другои , и тогда , как сам ги ным , из вле ка ясь в ла чи вать себя , в которую даже что бы ни\n",
      "\n",
      "input:   Чтобы жить честно\n",
      "output:  чтобы жить че стно жизнь , но я и це лыи год за те ря лся и пока жу свое все в душе и все время , то чтобы не слы шать того , что была , к которои вы думал , с пер ва в мое место , и у по крови теля\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обучение модели\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.01)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,  # 4 × 128 \n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"no\",\n",
    "    eval_steps=1 if DEVICE=='cpu' else 50,\n",
    "    logging_steps=1 if DEVICE=='cpu' else 50,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=hf_tokenizer,\n",
    "    callbacks=[\n",
    "        PromptCallback(\n",
    "            tokenizer=hf_tokenizer,\n",
    "            prompts=TEST_PROMPTS\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02ab7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del split_dataset, train_dataset, val_dataset, model, trainer\n",
    "gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae173c",
   "metadata": {},
   "source": [
    "## Post-train SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b40e1",
   "metadata": {},
   "source": [
    "### Подготовка модели QWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9bb76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_qwen = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, use_fast=True\n",
    ")\n",
    "\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "#model_qwen = model_qwen.cuda().half()\n",
    "model_qwen.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e41b49",
   "metadata": {},
   "source": [
    "### Ответы сырой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "883518e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input 1:\n",
      "сколько планет в нашей солнечной системе?\n",
      "Model Output 1:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "сколько планет в нашей солнечной системе?\n",
      "onnementi\n",
      "сколько планет в нашей солнечной системе? libertine i\n",
      "сколько планет в нашей солнечной системе? libertine\n",
      "сколько планет в нашей солнечной системе? libertine\n",
      "сколько планет в\n",
      "========================================\n",
      "Model Input 2:\n",
      "расскажи стих\n",
      "Model Output 2:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "расскажи стих\n",
      "itative и критично\n",
      "itative это\n",
      "itative это\n",
      "itative это тонкий\n",
      "itative это тонкий\n",
      "\n",
      "itative это тонкий\n",
      "itative это тонкий\n",
      "========================================\n",
      "Model Input 3:\n",
      "когда собирать крыжовник?\n",
      "Model Output 3:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "когда собирать крыжовник?\n",
      "tica\n",
      "Самые большие брексы у нас о них можно держать как 80 см.. сколько? nrw\n",
      "tica\n",
      "за сколько межкабельный шиповник\n",
      "tica\n",
      "60\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "def check_outputs(model, tokenizer, questtions):\n",
    "    def _generate_answer(model, question):\n",
    "        mess = [\n",
    "            {\"role\": \"user\",\n",
    "            \"content\": question}\n",
    "            ]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            mess,\n",
    "            return_tensors=\"pt\",\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        ).to(model_qwen.device)\n",
    "\n",
    "        output = model_qwen.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    for idx, q in enumerate(questtions):\n",
    "        print(f\"Model Input {idx+1}:\")\n",
    "        print(q)\n",
    "        print(f\"Model Output {idx+1}:\")\n",
    "        print(_generate_answer(model, q))\n",
    "        print(\"=\"*40)\n",
    "\n",
    "check_outputs(model_qwen, tokenizer_qwen, QUESTIONS[:3])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f759fa",
   "metadata": {},
   "source": [
    "### Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6b637496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51760"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"d0rj/alpaca-cleaned-ru\", split=\"train\")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1df52a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<|im_start|>system\\n'\n",
      "         'You are a helpful assistant.<|im_end|>\\n'\n",
      "         '<|im_start|>user\\n'\n",
      "         'Дайте три совета, как оставаться здоровым.<|im_end|>\\n'\n",
      "         '<|im_start|>assistant\\n'\n",
      "         '1. Соблюдайте сбалансированную и питательную диету. Убедитесь, что в '\n",
      "         'ваш рацион входят разнообразные фрукты и овощи, нежирный белок, '\n",
      "         'цельнозерновые продукты и полезные жиры. Это помогает обеспечить ваш '\n",
      "         'организм необходимыми питательными веществами для оптимального '\n",
      "         'функционирования и может помочь предотвратить хронические '\n",
      "         'заболевания.\\n'\n",
      "         '\\n'\n",
      "         '2. Занимайтесь регулярной физической активностью. Упражнения имеют '\n",
      "         'решающее значение для поддержания крепких костей, мышц и здоровья '\n",
      "         'сердечно-сосудистой системы. Старайтесь уделять не менее 150 минут '\n",
      "         'умеренным аэробным упражнениям или 75 минут интенсивным упражнениям '\n",
      "         'каждую неделю.\\n'\n",
      "         '\\n'\n",
      "         '3. Высыпайтесь. Достаточное количество качественного сна имеет '\n",
      "         'решающее значение для физического и психического благополучия. Он '\n",
      "         'помогает регулировать настроение, улучшать когнитивные функции и '\n",
      "         'поддерживает здоровый рост и иммунную функцию. Старайтесь спать 7-9 '\n",
      "         'часов каждую ночь.<|im_end|>\\n'}\n"
     ]
    }
   ],
   "source": [
    "def example_to_message(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {'role': 'user', 'content': example['instruction']},\n",
    "            {'role': 'assistant', 'content': example['output']}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "ds_chat = ds.map(\n",
    "    example_to_message,\n",
    "    batched=False,\n",
    "    remove_columns=['input', 'instruction', 'output', ],\n",
    ").map(\n",
    "    lambda x: {\n",
    "        'text': tokenizer_qwen.apply_chat_template(x['messages'],\n",
    "         tokenize=False)\n",
    "         },\n",
    "        remove_columns=['messages'],\n",
    "    )\n",
    "pprint(ds_chat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e65e3",
   "metadata": {},
   "source": [
    "### Обучение модели QWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfea8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "split_ds_chat = ds_chat.train_test_split(test_size=0.01)\n",
    "train_dataset = split_ds_chat['train']\n",
    "val_dataset = split_ds_chat['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637149ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2341' max='6408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2341/6408 25:40 < 44:38, 1.52 it/s, Epoch 1.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.849400</td>\n",
       "      <td>1.756764</td>\n",
       "      <td>1.833235</td>\n",
       "      <td>6507415.000000</td>\n",
       "      <td>0.622389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.587900</td>\n",
       "      <td>1.555425</td>\n",
       "      <td>1.571363</td>\n",
       "      <td>12936854.000000</td>\n",
       "      <td>0.655171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ClearCUDACallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def formatting_func(example):\n",
    "    return example[\"text\"]\n",
    "\n",
    "training_args_qwen = TrainingArguments(\n",
    "    output_dir=\"./checkpoints_qwen\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=3,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"no\",\n",
    "    eval_steps=1 if DEVICE=='cpu' else 1000,\n",
    "    logging_steps=1 if DEVICE=='cpu' else 1000,\n",
    "    fp16=True,\n",
    "    bf16=False,  \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model_qwen,\n",
    "    args=training_args_qwen,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    formatting_func=formatting_func, \n",
    "    processing_class=tokenizer_qwen,  \n",
    "    callbacks=[ClearCUDACallback()],\n",
    ")\n",
    "\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d17f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_qwen, split_ds_chat,  ds_chat, train_dataset, val_dataset \n",
    "gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c3f7738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"Model dtype:\", next(model_qwen.parameters()).dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1af8df41",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: training requires a train_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_train_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/trainer.py:1037\u001b[0m, in \u001b[0;36mTrainer._get_dataloader\u001b[0;34m(self, dataset, description, batch_size, sampler_fn, is_training, dataloader_key)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m   1033\u001b[0m         dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m   1034\u001b[0m             seed_worker, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_num_workers, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mprocess_index\n\u001b[1;32m   1035\u001b[0m         )\n\u001b[0;32m-> 1037\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataloader_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# Store the prepared dataloader for subsequent evaluations if using persistent workers.\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataloader_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_persistent_workers:\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:1477\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1467\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m   1468\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_device_map(obj)\n\u001b[1;32m   1469\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mNO\n\u001b[1;32m   1470\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACCELERATE_BYPASS_DEVICE_MAP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1471\u001b[0m     ):\n\u001b[1;32m   1472\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1473\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded with `device_map=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` in any distributed mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1474\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please rerun your script specifying `--num_processes=1` or by launching with `python \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mmyscript.py}}`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1475\u001b[0m         )\n\u001b[0;32m-> 1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   1478\u001b[0m     model_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m args:\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:674\u001b[0m, in \u001b[0;36mAccelerator.distributed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdistributed_type\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 674\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/accelerate/state.py:1219\u001b[0m, in \u001b[0;36mAcceleratorState.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# By this point we know that no attributes of `self` contain `name`,\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;66;03m# so we just modify the error message\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_attrs:\n\u001b[0;32m-> 1219\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1220\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`AcceleratorState` object has no attribute `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1221\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis happens if `AcceleratorState._reset_state()` was called and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1222\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man `Accelerator` or `PartialState` was not reinitialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1223\u001b[0m         )\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;66;03m# Raise a typical AttributeError\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcceleratorState\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized."
     ]
    }
   ],
   "source": [
    "batch = next(iter(trainer.get_train_dataloader()))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc37f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
