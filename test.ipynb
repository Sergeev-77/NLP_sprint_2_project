{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6bf891",
   "metadata": {},
   "source": [
    "# Проект большие языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa59ba",
   "metadata": {},
   "source": [
    "### Подготовка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61607157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast, LlamaConfig, LlamaForCausalLM, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW \n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e311b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4351634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE='cuda'\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"RussianNovels/corpus\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VOCAB_SIZE = 3000\n",
    "CYRILLIC_RE = re.compile(r\"^[\\p{IsCyrillic}\\s0-9.,!?—–:;\\\"'()«»…\\-]+$\")\n",
    "BATCH_SIZE = 12\n",
    "MAX_EPOCHS = 10\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP = 1.0\n",
    "SAVE_PATH = \"best_model.pt\"\n",
    "\n",
    "test_prompts = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\"\n",
    "] \n",
    "\n",
    "print(f\"{DEVICE=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07dc670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'RussianNovels' already exists and is not an empty directory.\n",
      "fatal: destination path 'alpaca-cleaned-ru' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/JoannaBy/RussianNovels.git\n",
    "!git clone https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56db64e",
   "metadata": {},
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404b8f6",
   "metadata": {},
   "source": [
    "### Подготовка корпуска текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3b3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлов в корпусе: 107\n"
     ]
    }
   ],
   "source": [
    "def collect_texts(root: Path):\n",
    "    texts = []\n",
    "    for path in root.rglob(\"*\"):\n",
    "        if path.suffix in {\".txt\"}:\n",
    "            try:\n",
    "                texts.append(path.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return texts\n",
    "\n",
    "raw_texts = collect_texts(DATA_DIR)\n",
    "raw_texts = list(set(raw_texts))\n",
    "print(f\"Файлов в корпусе: {len(raw_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c3d8316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложений после очистки: 523962\n"
     ]
    }
   ],
   "source": [
    "def filter_cyrillic_sentences(text: str):\n",
    "    '''фильтруем предложения с некириллическими символами'''\n",
    "    sentences = re.split(r\"[.!?]+\", text)\n",
    "    return [\n",
    "        s.strip()\n",
    "        for s in sentences\n",
    "        if s.strip() and CYRILLIC_RE.match(s.strip())\n",
    "    ]\n",
    "\n",
    "def normalize_punctuation(text: str) -> str:\n",
    "    '''нормализуемы пунктуацию'''\n",
    "    text = re.sub(r\"[!?]{2,}\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\.{3,}\", \"…\", text)\n",
    "    text = re.sub(r\",,{2,}\", \",\", text)\n",
    "    text = re.sub(r\"-{2,}\", \"-\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r'[«»]', '\"', text)\n",
    "    return text.strip()    \n",
    "\n",
    "clean_sentences = []\n",
    "\n",
    "for text in raw_texts:\n",
    "    sentences = filter_cyrillic_sentences(text)\n",
    "    sentences = [normalize_punctuation(s) for s in sentences]\n",
    "    clean_sentences.extend(sentences)\n",
    "\n",
    "print(f\"Предложений после очистки: {len(clean_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6095aab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка состава оставшихся спец символов\n",
      "symb | freg    | name\n",
      "' '  | 5716309 | SPACE\n",
      "','  |  853248 | COMMA\n",
      "'-'  |  279089 | HYPHEN-MINUS\n",
      "'\"'  |   49998 | QUOTATION MARK\n",
      "':'  |   44922 | COLON\n",
      "';'  |   32563 | SEMICOLON\n",
      "'–'  |   24641 | EN DASH\n",
      "')'  |    7897 | RIGHT PARENTHESIS\n",
      "'('  |    7337 | LEFT PARENTHESIS\n",
      "'—'  |    4435 | EM DASH\n",
      "'…'  |    2988 | HORIZONTAL ELLIPSIS\n",
      "'''  |    1152 | APOSTROPHE\n"
     ]
    }
   ],
   "source": [
    "print(\"Проверка состава оставшихся спец символов\")\n",
    "def is_letter_or_digit(ch: str) -> bool:\n",
    "    cat = unicodedata.category(ch)\n",
    "    return cat.startswith(\"L\") or cat.startswith(\"N\")\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for sent in clean_sentences:\n",
    "    for ch in sent:\n",
    "        if not is_letter_or_digit(ch):\n",
    "            counter[ch] += 1\n",
    "\n",
    "\n",
    "non_alnum = counter.most_common()\n",
    "print(f\"symb | freg    | name\")\n",
    "for ch, freq in non_alnum[:30]:\n",
    "    name = unicodedata.name(ch, \"UNKNOWN\")\n",
    "    print(f\"'{ch}'  | {freq:>7} | {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152d4382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чанков: 20039\n",
      "чанк  0: ) объявили смертный приговор шепотом Все встали, обмениваясь улыбками Седой судья, припав к его уху, подышав, сообщив, медленно отодвинулся, как будто отлипал Засим Цинцинната отвезли обратно в крепость Дорога обвивалась вокруг ее скалистого подножья и уходила под ворота: змея в расселину Был спокоен; однако его поддерживали во время путешествия по длинным коридорам, ибо он неверно ставил ноги, вроде ребенка, только что научившегося ступать, или точно куда проваливался, как человек, во сне увидевший, что идет по воде, но вдруг усомнившийся: да можно ли Тюремщик Родион долго отпирал дверь Цинциннатовой камеры, - не тот ключ, - всегдашняя возня Дверь наконец уступила Там, на койке, уже ждал адвокат - сидел, погруженный по плечи в раздумье, без фрака (забытого на венском стуле в зале суда, - был жаркий, насквозь синий день), - и нетерпеливо вскочил, когда ввели узника Но Цинциннату было не до разговоров Пускай одиночество в камере с глазком подобно ладье, дающей течь Все равно, - он заявил, что хочет остаться один, и, поклонившись, все вышли Итак - подбираемся к концу Правая, еще непочатая часть развернутого романа, которую мы, посреди лакомого чтенья, легонько ощупывали, машинально проверяя, много ли еще (и все радовала пальцы спокойная, верная толщина), вдруг, ни с того ни с сего, оказалась совсем тощей: несколько минут скорого, уже под гору чтенья - и ужасно Куча черешен, красно и клейко черневшая перед нами, обратилась внезапно в отдельные ягоды: вон та, со шрамом, подгнила, а эта сморщилась, ссохшись вокруг кости (самая же последняя непременно - тверденькая, недоспелая) Ужасно Цинциннат снял шелковую безрукавку, надел халат и, притоптывая, чтобы унять дрожь, пустился ходить по камере На столе белел чистый лист бумаги, и, выделяясь на этой белизне, лежал изумительно очиненный карандаш, длинный как жизнь любого человека, кроме Цинцинната, и с эбеновым блеском на каждой из шести граней Просвещенный потомок указательного перста\n",
      "чанк 10: Левее, почерком стремительным и чистым, без единой лишней линии: \"Обратите внимание, что когда они с вами говорят -\" - дальше, увы, было стерто Рядом - корявыми детскими буквами: \"Писателей буду штрафовать\" - и подпись: директор тюрьмы Еще можно было разобрать одну ветхую и загадочную строку: \"Смерьте до смерти, - потом будет поздно\" - Меня во всяком случае смерили, - сказал Цинциннат, тронувшись опять в путь и на ходу легонько постукивая костяшками руки по стенам - Как мне, однако, не хочется умирать Душа зарылась в подушку Ох, не хочется Холодно будет вылезать из теплого тела Не хочется, погодите, дайте еще подремать Двенадцать, тринадцать, четырнадцать Пятнадцать лет было Цинциннату, когда он начал работать в мастерской игрушек, куда был определен по причине малого роста По вечерам же упивался старинными книгами под ленивый, пленительный плеск мелкой волны, в плавучей библиотеке имени д-ра Синеокова, утонувшего как раз в том месте городской речки Бормотание цепей, плеск, оранжевые абажурчики на галерейке, плеск, липкая от луны водяная гладь, - и вдали, в черной паутине высокого моста, пробегающие огоньки Но потом ценные волюмы начали портиться от сырости, так что в конце концов пришлось реку осушить, отведя воду в Стропь посредством специально прорытого канала Работая в мастерской, он долго бился над затейливыми пустяками, занимался изготовлением мягких кукол для школьниц, - тут был и маленький волосатый Пушкин в бекеше, и похожий на крысу Гоголь в цветистом жилете, и старичок Толстой, толстоносенький, в зипуне, и множество других, например: застегнутый на все пуговки Добролюбов в очках без стекол Искусственно пристрастясь к этому мифическому девятнадцатому веку, Цинциннат уже готов был совсем углубиться в туманы древности и в них найти подложный приют, но другое отвлекло его внимание Там-то, на той маленькой фабрике, работала Марфинька, - полуоткрыв влажные губы, целилась ниткой в игольное ушко: \"Здравствуй, Цинциннатик\n"
     ]
    }
   ],
   "source": [
    "# разбиение на чанки\n",
    "chunks = []\n",
    "current = \"\"\n",
    "\n",
    "for sent in clean_sentences:\n",
    "    max_chars = 2000  # ~512 токенов\n",
    "    if len(current) + len(sent) < max_chars:\n",
    "        current += \" \" + sent\n",
    "    else:\n",
    "        chunks.append(current.strip())\n",
    "        current = sent\n",
    "\n",
    "if current:\n",
    "    chunks.append(current.strip())\n",
    "\n",
    "print(f\"Чанков: {len(chunks)}\")\n",
    "print(f\"чанк  0: {chunks[0]}\")\n",
    "print(f\"чанк 10: {chunks[10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cb3e6",
   "metadata": {},
   "source": [
    "### Обучение токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49e3cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Нормализация текста\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "# разбиение по пробелам \n",
    "tokenizer.pre_tokenizer = Whitespace() \n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"] \n",
    "    )\n",
    "tokenizer.train_from_iterator(chunks, trainer)\n",
    "# (BOS/EOS) \n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    pair=\"[BOS] $A [EOS] $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")), \n",
    "        ], \n",
    "        )\n",
    "# Сохранение\n",
    "tokenizer.save(\"bpe_ru_3k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8d1788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', 'пример', 'те', 'к', 'ста', 'для', 'про', 'вер', 'ки', 'то', 'ке', 'ни', 'за', 'тора', '[EOS]']\n",
      "[2, 1903, 80, 31, 123, 308, 106, 170, 100, 58, 220, 70, 81, 1680, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"Пример текста для проверки токенизатора\")\n",
    "print(encoded.tokens)\n",
    "print(encoded.ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8334a89",
   "metadata": {},
   "source": [
    "### Сборка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c6c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47506c69af2440b0a1d8cdf13561195f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20039 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517966c49ae749b69195b6e4ca8051c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20039 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"bpe_ru_3k.json\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    ")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return hf_tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "def add_labels(batch):\n",
    "    labels = []\n",
    "    for ids, mask in zip(batch[\"input_ids\"], batch[\"attention_mask\"]):\n",
    "        l = ids.copy()\n",
    "        l = [\n",
    "            token if m == 1 else -100\n",
    "            for token, m in zip(l, mask)\n",
    "        ]\n",
    "        labels.append(l)\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": chunks})\n",
    "\n",
    "dataset = dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ").map(add_labels, batched=True)\n",
    "\n",
    "dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c5204",
   "metadata": {},
   "source": [
    "### Подготовка модели LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62fe2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучаемых параметров: 128,934,912\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=2048,\n",
    "    rms_norm_eps=1e-6,\n",
    "    hidden_act=\"silu\",\n",
    "    tie_word_embeddings=True,\n",
    "    attention_bias=False,\n",
    "    rope_theta=10000.0,\n",
    "    use_cache=False,     \n",
    ")\n",
    "config.vocab_size = max(hf_tokenizer.get_vocab().values()) + 1\n",
    "config.pad_token_id = hf_tokenizer.pad_token_id\n",
    "config.bos_token_id = hf_tokenizer.bos_token_id\n",
    "config.eos_token_id = hf_tokenizer.eos_token_id\n",
    "\n",
    "model = LlamaForCausalLM(config).to(DEVICE)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Обучаемых параметров: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81028a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверим синхронизацию модели и текенизатора\n",
      "ок\n"
     ]
    }
   ],
   "source": [
    "print(\"Проверим синхронизацию модели и текенизатора\")\n",
    "assert model.config.vocab_size == max(hf_tokenizer.get_vocab().values()) + 1\n",
    "assert model.config.pad_token_id == hf_tokenizer.pad_token_id\n",
    "assert model.config.eos_token_id == hf_tokenizer.eos_token_id\n",
    "print(\"ок\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d2a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity-check начальной модели на инференсе\n",
      "при вет кры кры брат брат брат брат брат брат брат брат бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары бары\n"
     ]
    }
   ],
   "source": [
    "print(\"Sanity-check начальной модели на инференсе\")\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Привет\"\n",
    "inputs = hf_tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    return_token_type_ids=False\n",
    ").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=False,\n",
    "        eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(hf_tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ada5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимизатор\n",
    "def get_optim():\n",
    "    return AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optimizer = get_optim()\n",
    "\n",
    "# освобождение памяти\n",
    "def gc_collect():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b35f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сheck сходимости модели при обучение на малом датасете (2 предожения)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286d9a503fc04115ac901e73cabcc6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train check:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  20 loss: 8.74\n",
      "best model saved with best_loss=8.74\n",
      "epoch:  40 loss: 5.00\n",
      "best model saved with best_loss=5.00\n",
      "epoch:  60 loss: 5.15\n",
      "epoch:  80 loss: 4.00\n",
      "best model saved with best_loss=4.00\n",
      "epoch: 100 loss: 2.69\n",
      "best model saved with best_loss=2.69\n",
      "epoch: 120 loss: 0.52\n",
      "best model saved with best_loss=0.52\n",
      "epoch: 140 loss: 0.13\n",
      "best model saved with best_loss=0.13\n",
      "epoch: 160 loss: 0.05\n",
      "best model saved with best_loss=0.05\n",
      "epoch: 180 loss: 0.02\n",
      "best model saved with best_loss=0.02\n",
      "epoch: 200 loss: 0.01\n",
      "best model saved with best_loss=0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"Сheck сходимости модели при обучение на малом датасете (2 предожения)\")\n",
    "tiny_dataset = dataset.select(range(2))\n",
    "tiny_loader = DataLoader(tiny_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "progress_bar = tqdm(\n",
    "        range(201),\n",
    "        desc=f\"train check\",\n",
    "    )\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for step in progress_bar:\n",
    "    batch = next(iter(tiny_loader))\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "    loss = model(**batch).loss\n",
    "    progress_bar.set_description(f\"train check | loss: {loss:.2f}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step> 0 and step % 20 == 0:\n",
    "        progress_bar.write(f\"epoch: {step:>3} loss: {loss.item():.2f}\")\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "            progress_bar.write(f\"best model saved with {best_loss=:.2f}\")\n",
    "\n",
    "del tiny_dataset, tiny_loader, optimizer\n",
    "gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "614f969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сheck инференса модели после обучения на малом датасете\n",
      "input:   Все мысли, которые имеют огромные последствия\n",
      "output:  все мысли , которые име ют огром ные послед ствия ли ли все все все все все все все я де , су дья , как у я у ху , под в ы ша в , сообщи в , медленно нулся , как в нулся , за си м цин цин цин ната отве зли обра тно в кре\n",
      "\n",
      "input:   Сила войска зависит от его духа\n",
      "output:  сила вои ска зави си т от его ду ха объя ли все все все все все все все все все все все пред ками б ками плечи су су его при пав к , к его в , медленно ото дви нулся , медленно нулся , как м м цин цин ната отве зли обра тно в кре по\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Сheck инференса модели после обучения на малом датасете\")\n",
    "def check_inference(model, prompts: list):\n",
    "    model.eval()\n",
    "    for pr in prompts:\n",
    "        inputs = hf_tokenizer(pr, return_tensors=\"pt\").to(model.device)\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "        out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True, \n",
    "        eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    )\n",
    "        print(\"input:  \",  pr)\n",
    "        print(\"output: \", hf_tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "        print()\n",
    "\n",
    "\n",
    "check_inference(model, test_prompts[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af63d9b",
   "metadata": {},
   "source": [
    "### Обучение модели LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f88475a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbe0f27a59d4dcd94c2f999d817fb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 101 loss: 6.61\n",
      "epoch 1 step 201 loss: 6.34\n",
      "epoch 1 step 301 loss: 5.97\n",
      "epoch 1 step 401 loss: 5.69\n",
      "epoch 1 step 501 loss: 5.54\n",
      "epoch 1 step 601 loss: 5.20\n",
      "epoch 1 step 701 loss: 5.24\n",
      "epoch 1 step 801 loss: 5.14\n",
      "epoch 1 step 901 loss: 5.06\n",
      "epoch 1 step 1001 loss: 4.84\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     36\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_description(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), GRAD_CLIP)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "model.train()\n",
    "\n",
    "optimizer = get_optim()\n",
    "\n",
    "total_steps = len(train_loader) * MAX_EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "\n",
    "    progress_bar = tqdm(train_loader)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"]\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        progress_bar.set_description(\n",
    "            f\"train epoch {str(epoch+1):<2} step {str(step+1):<3} loss: {loss.item():.2f}\"\n",
    "        )\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step>0 and step % 100 == 0:\n",
    "            progress_bar.write(f\"epoch {epoch+1} step {step+1} loss: {loss.item():.2f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    progress_bar.write(f\"epoch {epoch+1} finished. avg_loss: {avg_loss:.2f}\")\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(model.state_dict(), SAVE_PATH)\n",
    "        progress_bar.write(f\"best model saved with {best_loss=:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e400e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/188 00:24 < 38:08, 0.08 it/s, Epoch 0.03/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=16,  # 16 × 8 = 128\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"no\",\n",
    "    eval_steps=10,\n",
    "    logging_steps=11,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # callbacks=[\n",
    "    #     PromptValidationCallback(\n",
    "    #         tokenizer=tokenizer,\n",
    "    #         prompts=[\n",
    "    #             \"Translate to English: Привет мир\",\n",
    "    #             \"Solve: 2 + 2 = ?\"\n",
    "    #         ],\n",
    "    #         expected_outputs=[\n",
    "    #             \"Hello world\",\n",
    "    #             \"4\"\n",
    "    #         ]\n",
    "    #     )\n",
    "    # ]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02ab7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21689d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
